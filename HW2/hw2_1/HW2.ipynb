{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15872,"status":"ok","timestamp":1709424942396,"user":{"displayName":"John Pascoe","userId":"15143252892239031302"},"user_tz":300},"id":"4fN88BynooLn","outputId":"968357f3-19f3-4ebb-a119-40dd8b3a1d0a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"markdown","source":["#**Utilities and Declarations**"],"metadata":{"id":"KVlwAFLEt-DT"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import random\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","from torch.utils.data import DataLoader\n","import json\n","import torch.optim as optim\n","import numpy as np\n","import tqdm\n","import os\n","import numpy as np\n","import pandas as pd\n","from collections import Counter\n","import glob\n","from tqdm import tqdm\n","import string\n","import time\n","from torch.utils.data import  Dataset\n","from torch.nn.utils.rnn import pad_sequence\n","\n","# Preprocess\n","def create_caption(label_json_path):\n","    f = open(label_json_path)\n","    data = json.load(f)\n","    filename=label_json_path[:-14]+'_newcaption.txt'\n","    if os.path.exists(filename):\n","        return filename\n","    else:\n","        with open(filename, 'a') as fb:\n","            fb.write('videoID'+';'+'Caption'+'\\n')\n","            for i in range(len(data)):\n","                for j in range(len(data[i]['caption'])):\n","                    fb.write(data[i]['id']+';'+data[i]['caption'][j]+'\\n')\n","        return filename\n","\n","def process(captions):\n","    rem_punct = str.maketrans('', '', string.punctuation)\n","    for i in range(len(captions)):\n","        line = captions[i]\n","        line = line.split()\n","\n","        line = [word.lower() for word in line]\n","\n","        line = [word.translate(rem_punct) for word in line]\n","\n","        line = [word for word in line if word.isalpha()]\n","\n","        captions[i] = ' '.join(line)\n","    return captions\n","\n","def numerize(caption, wtoi):\n","    return [wtoi[word] if word in wtoi else wtoi['<UNK>'] for word in caption.split()]\n","\n","class Vocabulary:\n","    def __init__(self, captions, freq_threshold=3):\n","        self.captions = process(captions)\n","        self.captions = captions\n","        self.itow = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n","        self.wtoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n","        self.freq_threshold = freq_threshold\n","\n","    def __len__(self):\n","        return len(self.itow)\n","\n","    def build_vocab(self,):\n","        vocab = {}\n","        idx=4\n","        for sentence in self.captions:\n","            for word in sentence.split():\n","                if word not in vocab:\n","                    vocab[word] = 1\n","\n","                else:\n","                    vocab[word]+=1\n","\n","                if vocab[word] == self.freq_threshold:\n","                    self.itow[idx] = word\n","                    self.wtoi[word] = idx\n","                    idx+=1\n","\n","class TrainDataset(Dataset):\n","    def __init__(self, feat_dir, label_json_path):\n","        self.df = pd.json_normalize(json.load(open(label_json_path)), meta=['id'], record_path=['caption'])\n","        self.df.columns = ['caption', 'id']\n","        self.feat_dir = feat_dir\n","        self.captions = self.df['caption']\n","        self.img_feat = self.df['id']\n","        self.vocab = Vocabulary(self.captions)\n","        self.vocab.build_vocab()\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","        caption = self.captions[index]\n","        feat = self.img_feat[index]\n","\n","        caption_tokens = caption.split()\n","\n","        caption_tokens = [word.lower() for word in caption_tokens]\n","\n","        caption_tokens = [word.translate(str.maketrans('', '', string.punctuation)) for word in caption_tokens]\n","\n","        caption_tokens = [word for word in caption_tokens if word.isalpha()]\n","\n","        numericalized_caption = [self.vocab.wtoi[\"<SOS>\"]]\n","        numericalized_caption += numerize(\" \".join(caption_tokens), self.vocab.wtoi)\n","        numericalized_caption.append(self.vocab.wtoi[\"<EOS>\"])\n","\n","        return feat, torch.Tensor(np.load(self.feat_dir+feat+'.npy')), torch.tensor(numericalized_caption)\n","\n","class TestDataset(Dataset):\n","    def __init__(self, feat_dir):\n","        self.feat_dir = feat_dir\n","        self.features = [f[:-4] for f in os.listdir(feat_dir) if f.endswith('.npy')]\n","\n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, index):\n","        idx = self.features[index]\n","        feat = os.path.join(self.feat_dir, f'{idx}.npy')\n","        return idx, torch.Tensor(np.load(feat))\n","\n","class Collate:\n","    def __init__(self, pad_idx):\n","        self.pad_idx = pad_idx\n","\n","    def __call__(self, batch):\n","        ids = [item[0] for item in batch]\n","        feats = [item[1].unsqueeze(0) for item in batch]\n","        feats = torch.cat(feats, dim=0)\n","        targets = [item[2].clone().detach() for item in batch]\n","        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n","\n","        return ids, feats, torch.transpose(targets, 0, 1)\n","\n","# Model\n","class Attention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Attention, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.W1 = nn.Linear(2*hidden_size, hidden_size)\n","        self.W2 = nn.Linear(hidden_size, hidden_size)\n","        self.W3 = nn.Linear(hidden_size, hidden_size)\n","        self.to_weight = nn.Linear(hidden_size, 1, bias=False)\n","\n","    def forward(self, hidden_state, encoder_outputs):\n","        batch_size, seq_len, feat_n = encoder_outputs.size()\n","        hidden_state = hidden_state.view(batch_size, 1, feat_n).repeat(1, seq_len, 1)\n","        matching_inputs = torch.cat((encoder_outputs, hidden_state), 2).view(-1, 2*self.hidden_size)\n","\n","        x = self.W1(matching_inputs)\n","        x = self.W2(x)\n","        x = self.W3(x)\n","        attention_weights = self.to_weight(x)\n","        attention_weights = attention_weights.view(batch_size, seq_len)\n","        attention_weights = F.softmax(attention_weights, dim=1)\n","        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n","\n","        return context\n","\n","class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, dropout=0.2):\n","        super(EncoderRNN, self).__init__()\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","\n","        self.compress = nn.Linear(in_features=input_size, out_features=hidden_size)\n","        self.dropout = nn.Dropout(dropout)\n","        self.gru = nn.GRU(input_size=hidden_size, hidden_size=hidden_size, batch_first=True)\n","\n","    def forward(self, input):\n","        batch_size, seq_len, feat_n = input.size()\n","        input = input.view(-1, feat_n)\n","        input = self.compress(input)\n","        input = self.dropout(input)\n","        input = input.view(batch_size, seq_len, self.hidden_size)\n","        output, hidden_state = self.gru(self.dropout(input))\n","\n","        return output, hidden_state\n","\n","\n","\n","\n","class DecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, vocab_size, embed_dim, helper=None, dropout=0.2):\n","        super(DecoderRNN, self).__init__()\n","        self.hidden_size, self.vocab_size, self.embed_dim, self.helper = hidden_size, vocab_size, embed_dim, helper\n","\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","        self.dropout = nn.Dropout(dropout)\n","        self.gru = nn.GRU(hidden_size+embed_dim, hidden_size, batch_first=True)\n","        self.attention = Attention(hidden_size)\n","        self.linear = nn.Linear(hidden_size,vocab_size)\n","\n","\n","    def forward(self, encoder_hidden=None, encoder_output=None, targets=None, mode=None, teacher_ratio=0.7):\n","        batch_size, _, _ = encoder_output.size()\n","        hidden_state = (self.init_state(batch_size).unsqueeze(0)).to(device)\n","        seq_logProb = []\n","        caption_preds = []\n","        targets = self.embedding(targets)\n","        _, seq_len, _ = targets.size()\n","\n","        embed = targets[:, 0]\n","        for i in range(seq_len-1):\n","            context = self.attention(hidden_state, encoder_output)\n","            gru_input = torch.cat([embed, context], dim=1).unsqueeze(1)\n","            gru_output, hidden_state = self.gru(gru_input, hidden_state)\n","            logprob = self.linear(self.dropout(gru_output.squeeze(1)))\n","            seq_logProb.append(logprob.unsqueeze(1))\n","\n","            use_teacher_forcing = True if random.random() < teacher_ratio else False\n","            if use_teacher_forcing:\n","                embed = targets[:, i+1]\n","            else:\n","                decoder_input = logprob.unsqueeze(1).max(2)[1]\n","                embed = self.embedding(decoder_input).squeeze(1)\n","\n","        seq_logProb = torch.cat(seq_logProb, dim=1)\n","        caption_preds = seq_logProb.max(2)[1]\n","        return seq_logProb, caption_preds\n","\n","\n","    def inference(self, encoder_hidden, encoder_output, vocab):\n","        batch_size, _, _ = encoder_output.size()\n","        hidden_state = (self.init_state(batch_size).unsqueeze(0)).to(device)\n","        decoder_input = torch.tensor(1).view(1,-1).to(device)\n","        seq_logProb = []\n","        caption_preds = []\n","        max_seq_len = 30\n","\n","        for i in range(max_seq_len-1):\n","            embed = self.embedding(decoder_input).squeeze(1)\n","            context = self.attention(hidden_state, encoder_output)\n","            gru_input = torch.cat([embed, context], dim=1).unsqueeze(1)\n","            gru_output, hidden_state = self.gru(gru_input, hidden_state)\n","            logprob = self.linear(gru_output.squeeze(1))\n","            seq_logProb.append(logprob.unsqueeze(1))\n","            decoder_input = logprob.unsqueeze(1).max(2)[1]\n","\n","            if vocab.itow[decoder_input.item()] == \"<EOS>\":\n","                break\n","\n","        seq_logProb = torch.cat(seq_logProb, dim=1)\n","        caption_preds = seq_logProb.max(2)[1]\n","        return seq_logProb, caption_preds\n","\n","\n","    def init_state(self, batch_size):\n","        return torch.zeros((batch_size, self.hidden_size))\n","\n","\n","class S2VTMODEL(nn.Module):\n","    def __init__(self, input_size, hidden_size, vocab_size, embed_dim):\n","        super(S2VTMODEL, self).__init__()\n","        self.encoder = EncoderRNN(input_size, hidden_size)\n","        self.decoder = DecoderRNN(hidden_size, vocab_size, embed_dim)\n","\n","\n","    def forward(self, features, target_captions=None, mode=None):\n","        encoder_outputs, encoder_hidden = self.encoder(features)\n","        if mode == 'train':\n","            seq_logProb, caption_preds = self.decoder(encoder_hidden=encoder_hidden, encoder_output=encoder_outputs, targets=target_captions, mode=mode)\n","        elif mode == 'test':\n","            seq_logProb, caption_preds = self.decoder.inference(encoder_hidden=encoder_hidden, encoder_output=encoder_outputs, vocab=None)\n","        else:\n","            raise KeyError('mode is not valid')\n","        return seq_logProb, caption_preds"],"metadata":{"id":"SgtVY3KAtkL3","executionInfo":{"status":"ok","timestamp":1709426376651,"user_tz":300,"elapsed":148,"user":{"displayName":"John Pascoe","userId":"15143252892239031302"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["#**Train Model**"],"metadata":{"id":"sSC_N793uF07"}},{"cell_type":"markdown","source":["*This can take over an hour*"],"metadata":{"id":"c-tX6h-9ucEN"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"FvrQKM3Sos4N","executionInfo":{"status":"error","timestamp":1709425080915,"user_tz":300,"elapsed":31672,"user":{"displayName":"John Pascoe","userId":"15143252892239031302"}},"outputId":"16f1929f-55e1-4c69-8b2d-7b9c9e0af40f"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-25d600571709>\u001b[0m in \u001b[0;36m<cell line: 309>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0mtrainloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch: {epoch+1}  Step: {step+1}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-25d600571709>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mnumericalized_caption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwtoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<EOS>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumericalized_caption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTestDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Train\n","captions_json = '/content/drive/MyDrive/CPSC 8430 Deep Learning/HW2/MLDS_hw2_1_data/training_label.json'\n","feat_folder = '/content/drive/MyDrive/CPSC 8430 Deep Learning/HW2/MLDS_hw2_1_data/training_data/feat/'\n","\n","train_dataset = TrainDataset(feat_folder, captions_json)\n","trainloader = DataLoader(dataset=train_dataset, batch_size=64, num_workers=0, shuffle=True, collate_fn=Collate(pad_idx=0))\n","\n","learning_rate = 1e-4\n","epochs=23\n","input_size=4096\n","hidden_size=512\n","vocabs = train_dataset.vocab\n","vocab_size=len(vocabs.wtoi)\n","embed_dim=256\n","LOAD_MODEL = False\n","\n","if LOAD_MODEL == True:\n","    model = torch.load('/content/drive/MyDrive/CPSC 8430 Deep Learning/HW2/models/Model1.h5')\n","    model.to(device)\n","else:\n","    model = S2VTMODEL(input_size=input_size, hidden_size=hidden_size, vocab_size=vocab_size, embed_dim=embed_dim)\n","    model.to(device)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=vocabs.wtoi[\"<PAD>\"])\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay = 1e-5)\n","\n","\n","modelloss=[]\n","\n","model.train()\n","step=0\n","bestloss = 20\n","for epoch in range(0, epochs):\n","    trainloss = 0\n","    step = 0\n","    for idx, features, captions in trainloader:\n","        if step%10==0:\n","            print(f'Epoch: {epoch+1}  Step: {step+1}')\n","        step+=1\n","        features = features.to(device)\n","        captions = captions.to(device)\n","        optimizer.zero_grad()\n","        outputs, seq_pred = model(features, captions, 'train')\n","        targets = captions[:, 1:]\n","        loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n","        trainloss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","    avgloss = trainloss/len(trainloader)\n","    modelloss.append(avgloss)\n","    if epoch%1==0:\n","        print('Epoch: {}    TotalLoss: {}'.format(epoch, avgloss))\n","    if avgloss < bestloss:\n","        bestloss = avgloss\n","        try:\n","          torch.save(model, f\"{'Model1'}.h5\")\n","        except:\n","          print(model)\n"]},{"cell_type":"markdown","source":["#**BLEU Evaluation Init**"],"metadata":{"id":"3mZ96iAcuKl2"}},{"cell_type":"code","source":["import math\n","import operator\n","import sys\n","import json\n","from functools import reduce\n","def count_ngram(candidate, references, n):\n","    clipped_count = 0\n","    count = 0\n","    r = 0\n","    c = 0\n","    for si in range(len(candidate)):\n","        # Calculate precision for each sentence\n","        ref_counts = []\n","        ref_lengths = []\n","        # Build dictionary of ngram counts\n","        for reference in references:\n","            ref_sentence = reference[si]\n","            ngram_d = {}\n","            words = ref_sentence.strip().split()\n","            ref_lengths.append(len(words))\n","            limits = len(words) - n + 1\n","            # loop through the sentance consider the ngram length\n","            for i in range(limits):\n","                ngram = ' '.join(words[i:i+n]).lower()\n","                if ngram in ngram_d.keys():\n","                    ngram_d[ngram] += 1\n","                else:\n","                    ngram_d[ngram] = 1\n","            ref_counts.append(ngram_d)\n","        # candidate\n","        cand_sentence = candidate[si]\n","        cand_dict = {}\n","        words = cand_sentence.strip().split()\n","        limits = len(words) - n + 1\n","        for i in range(0, limits):\n","            ngram = ' '.join(words[i:i + n]).lower()\n","            if ngram in cand_dict:\n","                cand_dict[ngram] += 1\n","            else:\n","                cand_dict[ngram] = 1\n","        clipped_count += clip_count(cand_dict, ref_counts)\n","        count += limits\n","        r += best_length_match(ref_lengths, len(words))\n","        c += len(words)\n","    if clipped_count == 0:\n","        pr = 0\n","    else:\n","        pr = float(clipped_count) / count\n","    bp = brevity_penalty(c, r)\n","    return pr, bp\n","\n","\n","def clip_count(cand_d, ref_ds):\n","    \"\"\"Count the clip count for each ngram considering all references\"\"\"\n","    count = 0\n","    for m in cand_d.keys():\n","        m_w = cand_d[m]\n","        m_max = 0\n","        for ref in ref_ds:\n","            if m in ref:\n","                m_max = max(m_max, ref[m])\n","        m_w = min(m_w, m_max)\n","        count += m_w\n","    return count\n","\n","\n","def best_length_match(ref_l, cand_l):\n","    \"\"\"Find the closest length of reference to that of candidate\"\"\"\n","    least_diff = abs(cand_l-ref_l[0])\n","    best = ref_l[0]\n","    for ref in ref_l:\n","        if abs(cand_l-ref) < least_diff:\n","            least_diff = abs(cand_l-ref)\n","            best = ref\n","    return best\n","\n","\n","def brevity_penalty(c, r):\n","    if c > r:\n","        bp = 1\n","    else:\n","        bp = math.exp(1-(float(r)/c))\n","    return bp\n","\n","\n","def geometric_mean(precisions):\n","    return (reduce(operator.mul, precisions)) ** (1.0 / len(precisions))\n","\n","\n","def BLEU(s,t,flag = False):\n","\n","    score = 0.\n","    count = 0\n","    candidate = [s.strip()]\n","    if flag:\n","        references = [[t[i].strip()] for i in range(len(t))]\n","    else:\n","        references = [[t.strip()]]\n","    precisions = []\n","    pr, bp = count_ngram(candidate, references, 1)\n","    precisions.append(pr)\n","    score = geometric_mean(precisions) * bp\n","    return score\n","\n","\n"],"metadata":{"id":"6EYTRw_crhyC","executionInfo":{"status":"ok","timestamp":1709425191156,"user_tz":300,"elapsed":126,"user":{"displayName":"John Pascoe","userId":"15143252892239031302"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_s1xFIrCyOR3"},"source":["#**Test Model**"]},{"cell_type":"markdown","source":["*Can be utilized with a pre-trained model called Model1.h5 in a ./models folder*"],"metadata":{"id":"IDk-aMVpujQH"}},{"cell_type":"code","execution_count":23,"metadata":{"id":"Fk9BUE4QyQGn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709425540496,"user_tz":300,"elapsed":27221,"user":{"displayName":"John Pascoe","userId":"15143252892239031302"}},"outputId":"e4cefa52-d9b5-4195-b7a4-628ebe8c92ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Average bleu score is 0.6814645471780354\n"]}],"source":["#from model import Attention, EncoderRNN, DecoderRNN, S2VTMODEL\n","#from preprocess import TrainDataset, TestDataset, Vocabulary, Collate\n","from torch.utils.data import DataLoader\n","import torch\n","#from bleu_eval import BLEU\n","import json\n","import pandas as pd\n","import sys\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","test_dataset = TestDataset('/content/drive/MyDrive/CPSC 8430 Deep Learning/HW2/MLDS_hw2_1_data/testing_data/feat/')\n","testloader = DataLoader(dataset=test_dataset, batch_size=1, num_workers=0, shuffle=False)\n","\n","#create vocabulary dictionary from training captions\n","df = pd.json_normalize(json.load(open('/content/drive/MyDrive/CPSC 8430 Deep Learning/HW2/MLDS_hw2_1_data/training_label.json')), meta=['id'], record_path=['caption'])\n","df.columns = ['caption', 'id']\n","caption = df['caption']\n","vocabs = Vocabulary(caption)\n","vocabs.build_vocab()\n","\n","\n","if not torch.cuda.is_available():\n","    model = torch.load('/content/drive/MyDrive/CPSC 8430 Deep Learning/HW2/models/Model1.h5', map_location=device)\n","else:\n","    model = torch.load('/content/drive/MyDrive/CPSC 8430 Deep Learning/HW2/models/Model1.h5')\n","\n","ids=[]\n","\n","for idx, features in testloader:\n","    model.eval()\n","    with torch.no_grad():\n","\n","        features, state = model.encoder(features.to(device))\n","        logprobs,caps = model.decoder.inference(encoder_hidden=None, encoder_output=features, vocab=vocabs)\n","        caption = ' '.join([vocabs.itow[i] for i in caps[0].tolist()])\n","        caption = caption.split('<EOS>')[0]\n","        if idx not in ids:\n","            ids.append(idx)\n","            with open('/content/drive/MyDrive/CPSC 8430 Deep Learning/HW2/output.txt', 'a') as f:\n","                f.write(idx[0]+','+caption+'\\n')\n","\n","\n","\n","test = json.load(open(f'/content/drive/MyDrive/CPSC 8430 Deep Learning/HW2/MLDS_hw2_1_data/testing_label.json','r'))\n","output = '/content/drive/MyDrive/CPSC 8430 Deep Learning/HW2/output.txt'\n","result = {}\n","with open(output,'r') as f:\n","    for line in f:\n","        line = line.rstrip()\n","        comma = line.index(',')\n","        test_id = line[:comma]\n","        caption = line[comma+1:]\n","        result[test_id] = caption\n","\n","bleu=[]\n","for item in test:\n","    score_per_video = []\n","    captions = [x.rstrip('.') for x in item['caption']]\n","    score_per_video.append(BLEU(result[item['id']],captions,True))\n","    bleu.append(score_per_video[0])\n","average = sum(bleu) / len(bleu)\n","print(\"Average bleu score is \" + str(average))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyO0oCh1McDcO7ybU3ZZAfVG"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}